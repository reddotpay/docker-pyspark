# FROM python:3.7-slim
FROM python:3.7-slim-buster
LABEL maintainer=" ken"

# Default values can be overridden at build time
# (ARGS are in lower case to distinguish them from ENV)
ARG AIRFLOW_PIP_VERSION=20.2.4
ARG spark_version="3.1.2"
ARG hadoop_version="3.2"

ENV APACHE_SPARK_VERSION="${spark_version}"
ENV HADOOP_VERSION="${hadoop_version}"
ENV JAVA_HOME=/usr/java/jdk1.8.0_281

WORKDIR /usr/local

# Install the linux packages
RUN apt-get update && apt-get install -y \
    gcc \
    openssl \
    libssl-dev \
    groff \
    jq \
    make \
    zip \
    unzip \
    curl \
    wget \
    tar \
    nano

# Install the java-jdk 
RUN mkdir -p /usr/java && \
    wget --no-cookies --no-check-certificate --header "Cookie: oraclelicense=accept-securebackup-cookie" https://javadl.oracle.com/webapps/download/GetFile/1.8.0_281-b09/89d678f2be164786b292527658ca1605/linux-i586/jdk-8u281-linux-x64.tar.gz && \
    tar -zxvf jdk-8u281-linux-x64.tar.gz && \
    mv jdk1.8.0_281 ${JAVA_HOME}

# Install the AWS CLI in the container
RUN mkdir -p /aws/install && \
    curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip" \
    && unzip awscliv2.zip \
    && ./aws/install

RUN rm -rf \
    awscliv2

# Spark installation
# Using the preferred mirror to download Spark
# hadolint ignore=SC2046
RUN wget -q $(wget -qO- https://www.apache.org/dyn/closer.lua/spark/spark-${APACHE_SPARK_VERSION}/spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\?as_json | \
    python -c "import sys, json; content=json.load(sys.stdin); print(content['preferred']+content['path_info'])") && \
    tar xzf "spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" -C /usr/local --owner root --group root --no-same-owner && \
    rm "spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz"

# # Configure Spark
ENV SPARK_HOME=/usr/local/spark
ENV SPARK_OPTS="--driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info" \
    PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH \
    PATH=$SPARK_HOME/bin:$SPARK_HOME/python:$PATH

RUN ln -s "spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}" spark

# Download the necessary JDBC file in spark/jars
RUN wget -O snowflake-jdbc-3.13.5.jar https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.13.5/snowflake-jdbc-3.13.5.jar && \
    mv snowflake-jdbc-3.13.5.jar spark/jars/snowflake-jdbc-3.13.5.jar
RUN wget -O spark-snowflake_2.12-2.9.0-spark_3.1.jar https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.12/2.9.0-spark_3.1/spark-snowflake_2.12-2.9.0-spark_3.1.jar && \
    mv spark-snowflake_2.12-2.9.0-spark_3.1.jar spark/jars/spark-snowflake_2.12-2.9.0-spark_3.1.jar
RUN wget -O mysql-connector-java-8.0.25.jar https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.25/mysql-connector-java-8.0.25.jar && \
    mv mysql-connector-java-8.0.25.jar spark/jars/mysql-connector-java-8.0.25.jar
RUN wget -O aws-java-sdk-1.12.22.jar https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.12.22/aws-java-sdk-1.12.22.jar && \
    mv aws-java-sdk-1.12.22.jar spark/jars/aws-java-sdk-1.12.22.jar
RUN wget -O hadoop-aws-3.2.0.jar https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar && \
    mv hadoop-aws-3.2.0.jar spark/jars/hadoop-aws-3.2.0.jar
RUN wget -O aws-java-sdk-bundle-1.11.874.jar https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.874/aws-java-sdk-bundle-1.11.874.jar && \
    mv aws-java-sdk-bundle-1.11.874.jar spark/jars/aws-java-sdk-bundle-1.11.874.jar

# Python commands to install the corresponding library
RUN pip install --no-cache-dir --upgrade "pip==${AIRFLOW_PIP_VERSION}"
RUN python -m pip install \
    autopep8 \
    virtualenv \
    pylint \
    coverage \
    sphinx \
    sphinx-rtd-theme \
    boto3 \
    s3pypi \
    pandas==0.23.4 \
    snowflake-connector-python \
    google-api-python-client \
    google-auth-httplib2 \
    google-auth-oauthlib \
    py4j

COPY ["switchRole.sh", "getParamStore.sh", "/var/"]

CMD ["/bin/sh"]
